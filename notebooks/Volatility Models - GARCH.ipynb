{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8OgHhD8MpE-"
   },
   "source": [
    "### Week 10 - Volatility Models GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwNBLL1rMuum"
   },
   "source": [
    "### 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y4OFTwKMxJB"
   },
   "source": [
    "Switch to the mean to the variance.\n",
    "\n",
    "Useful in risk management, VaR, option pricing, long-run marginal expected short-fall, predictive regressions, interaction between volatility and the real economy.\n",
    "\n",
    "Models of volatility:\n",
    "*   Generalised Autoregressive Conditional Heteroskedasticity (GARCH)\n",
    "*   Realised Volatility/Variance (RV)\n",
    "*   Stochastic Volatility (SV)\n",
    "*   Implicit Volatility (IV)\n",
    "*   Range Volatility\n",
    "\n",
    "We see with return series tends to have bursts of volatility with positive autocorrelation in the variance, with quiet and more volatile periods, almost like 2 regimes. You are now saying the variance is non-constant over time, hence heteroskedastic.\n",
    "\n",
    "Descriptive statistics are based under the assumption of a constant variance, which suggests misspecification.\n",
    "\n",
    "Autocorrelation of returns shows no significant AR relationship, based on the ACF and PACF, i.e. no predictability.\n",
    "\n",
    "However, we see autocorrelation in squared returns, i.e. predictability.\n",
    "Note: this is the variance of returns provided the mean return is zero. If it is ver small, we could treat squared returns as a reasonable approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UOSoitm7AD2d"
   },
   "source": [
    "### 2. Testing for Constant or Time-Varying Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGKYzc5tAG8F"
   },
   "source": [
    "Obtain squared-demeaned return series i.e. variance.\n",
    "\n",
    "Do a AR equation and check the ACF/PACF.\n",
    "\n",
    "Test the following hypothesesis:\n",
    "*   Null all coefficients for AR terms are jointly 0 (constant variance)\n",
    "*   At least one restriction is violated (time-varying variance)\n",
    "\n",
    "This test represents a preliminary test for ARCH and denoted as ARCH(q).\n",
    "\n",
    "Returns are represented as:\n",
    "$r_t = \\phi_0 + u_t$\n",
    "\n",
    "Squared demeaned returns is the variance of the series:\n",
    "$u_t^2 = (r_t - \\phi_0)^2$\n",
    "\n",
    "In practice, we would estimate the above return model and extract the model residuals $\\hat u_t$.\n",
    "\n",
    "Then you estimate a ARCH(q) or a AR(q) model for the variance:\n",
    "\n",
    "$\\hat u_t^2 = \\delta_0 + \\delta_1 \\hat u_{t-1}^2 + ... + \\delta_q \\hat u_{t-q}^2 + v_t$\n",
    "\n",
    "Then you compute the test statistic: \n",
    "\n",
    "$ARCH = TR^2$ where T is the sample size and $R^2$ is the coefficient of determination from the regression.\n",
    "\n",
    "Under the null hypothesis of constant variance, the test statistic is distributed under a chi-squared distribution with q degrees of freedom. We can estract the appropriate critical values and p-values from this distribution.\n",
    "\n",
    "Given this, if the test statistic p-value is greater than 0.05, we fail to reject the null at the 5% level. If it is less than 0.05, we reject the null in favour of the alternative. This procedure is formally known as the Lagrange multiplier test.\n",
    "\n",
    "Eviews: Quick / Estimate Equation / r c / View / Residual Diagnostics / Heteroskedasticity Tests / ARCH / q / Ok.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQLvaeaADzsa"
   },
   "source": [
    "### 3. Autoregressive Conditional Heteroskedasticity (ARCH) Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lA4XxlFND4zo"
   },
   "source": [
    "ARCH models capture autocorrelation in the variance. \n",
    "\n",
    "The variance is determined by the size of previous shocks.\n",
    "\n",
    "e.g. the ARCH(1) model is:\n",
    "\n",
    "$r_t = \\phi_0 + u_t$\n",
    "\n",
    "$h_t = \\alpha_0 + \\alpha_1 u_{t-1}^2$\n",
    "\n",
    "$u_t -> N(0,h_t)$\n",
    "\n",
    "THe key features is that the conditional mean is a constant, and the distirbution of $u_t$ is normally distributed.\n",
    "\n",
    "$f(r_t|r_{t-1},r_{t-2},..,\\theta) = \\frac{1}{\\sqrt{2\\pi h_t}} \\exp(-\\frac{(r_t - \\phi_0)^2}{2 h_t})$\n",
    "\n",
    "Interpretation is that small shocks $u_{t-1}$ will result in a small variance $h_t$ and vice-versa. Large shocks will result in an increase in variance in the next period if $\\alpha_1 > 0$.\n",
    "\n",
    "The \"News Impact Curve\" is a plot of $h_t$ against $u_t$. We see shocks of the same magnitude have the same effect on $h_t$ (no asymmetry effects).\n",
    "\n",
    "A very specifal case of the ARCH(1) model is a constant variance $\\alpha_1 = 0$. \n",
    "\n",
    "The model can be generalised to have more lags.\n",
    "\n",
    "Estimating the ARCH model parameters:\n",
    "\n",
    "$\\theta = [\\phi_0, \\alpha_0, \\alpha_1, .., \\alpha_q]$\n",
    "\n",
    "We estimate using maximum likelihood using an iterative alogorithm. Given conditional normality, the log-likelihood function is:\n",
    "\n",
    "$\\log L(\\theta) = \\frac{1}{T} \\sum^T_{t=1} \\log (r_t|r_{t-1},r_{t-2},...,\\theta)$\n",
    "\n",
    "$-\\frac{1}{2} \\log (2\\pi) - \\frac{1}{2T}\\sum^T_{t=1} \\log h_t - \\frac{1}{2T} \\sum^T_{t=1} \\frac{u^2_t}{h_t}$\n",
    "\n",
    "Where $u_t = y_t - \\phi_0$\n",
    "\n",
    "$h_t = \\alpha_0 + \\sum^q_{i = 1} \\alpha_i u^2_{t-1}$\n",
    "\n",
    "Eviews: Quick / Estimate Equation / r c / Estimation settings: ARCH / ARCH order 1 / GARCH order 0 / OK\n",
    "\n",
    "View / GARCH Graph / Conditional Variance/ Proc / Make GARCH Variance Series / OK\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZtc8vkaS6Ta"
   },
   "source": [
    "### 4. Is the Model a Good Fit? Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ei6XYQcaS-S-"
   },
   "source": [
    "We have estimated the ARCH model.\n",
    "\n",
    "Is the model any good?\n",
    "\n",
    "If the model is specified correctly, then the standardised reisiduals \n",
    "\n",
    "$z_t = \\frac{u_t}{\\sqrt{h_t}}$, the variance of it should be 1, i.e. $var(z_t) = 1$ and that there should be no evidence of autocorrelation in the variance. \n",
    "\n",
    "This is because our model assumptions is that the error term is normally distributed with 0 mean and variance $h_t$\n",
    "\n",
    "Also, we want to check if we have the true process. E.g. What if the true process is an ARCH(2) when we have an ARCH(1)?\n",
    "\n",
    "$Var(z_t) = \\frac{var(u_t)}{h_t}$\n",
    "\n",
    "If the correct model specification is done, that the true $var(u_t)$ and $var(h_t)$ are the same.\n",
    "\n",
    "However, if for example the $var(u_t)$ is ARCH(3) and $h_t$ is ARCH(1), then the ratio would be ARCH(2) as the variance expressions in the numerator and denominator would not cancel out.\n",
    "\n",
    "$u_t$ is the true variance and $h_t$ is the specified variance from the model.\n",
    "\n",
    "Hence, we want to test the following hypothesis:\n",
    "*   Null: model is correctly specified where $z_t$ variance is constant\n",
    "*   Alternative: model is misspecified and $z_t$ is time-varying\n",
    "\n",
    "i.e. We should see white-noise standardised residuals.\n",
    "\n",
    "Eviews: View / Residual Diagnostics / ARCH LM Test / Number of Lags: 1 (is there another lag in the true model?)\n",
    "\n",
    "We would get the ARCH test statistic and see its relevant p-value based on the chi-square distribution under trhe null. If less than 0.05, we reject the null of correct specification and conclude that the ARCH variance specification has higher order dynamics than ARCH(1) for example.\n",
    "\n",
    "We keep estimating a new model, reapplying the test until we can no longer reject the null.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o62QBTzBWh4Q"
   },
   "source": [
    "### 5. Generalised ARCH (GARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAYrV_xdWkeS"
   },
   "source": [
    "When estimating ARCH models, you find that you generally need a fairly long q. i.e. Volatility seems to have long-memory and is persistent. \n",
    "\n",
    "This can mean using a lot of degrees of freedom and lags.\n",
    "\n",
    "Like MA, we can capture this persistence where variance tends to cluster in periods while controlling for the number of parameters, we add an additional explanatory variable in the lagged conditional variance. \n",
    "\n",
    "So the GARCH term is responsible for capturing volatility persistence and memory features of volatiliy, which means that many lagged volatilities has predictability on current and future volatility. The GARCH terms allow for an infinite ARCH structure. Same ARMA for volatility.\n",
    "\n",
    "$h_t = \\alpha_0 + \\alpha_1 u^2_{t-1} + \\beta_1 h_{t-1}$\n",
    "\n",
    "We can show GARCH(1,1) to be a infinite ARCH model with just this additional parameter $\\beta_1$.\n",
    "\n",
    "We have the GARCH(1,1) model:\n",
    "$(1-\\beta_1 L)h_t = \\alpha_0 + \\alpha_1 u^2_{t-1}$\n",
    "\n",
    "Assuming $|\\beta_1|<1$, then we can invert the term:\n",
    "$h_t = (1-\\beta_1L)^{-1} \\alpha_0 + (1-\\beta_1L)^{-1} u^2_{t-1}$\n",
    "\n",
    "$= \\frac{\\alpha_0}{1-\\beta_1}+\\alpha_1(1+\\beta_1 L + \\beta_1^2 L^2 + ...)u^2_{t-1}$\n",
    "\n",
    "$= \\frac{\\alpha_0}{1-\\beta_1}+(\\alpha_1 u^2_{t-1} + \\alpha_1 \\beta_1 u^2_{t-2} + \\alpha_1 \\beta^2_1 u^2_{t-3} + ...)$\n",
    "\n",
    "Hence, we have an infinite order ARCH model.\n",
    "\n",
    "The $\\beta_1$ effectively tells us the effect of past shocks. The bigger the number, the bigger the strength of previous periods and thus, the longer the memory.\n",
    "\n",
    "We can allow for q lags of the ARCH term and p lags of the GARCH terms, resulting in GARCH(p,q) model.\n",
    "\n",
    "Again model assumptions have normal error term and conditional mean being constant.\n",
    "\n",
    "The GARCH(1,1) model generally is the best model. The estimates of $\\beta_1$ is around 0.9 and the $\\alpha_1$ term is around 0.05.\n",
    "\n",
    "Special case is ARCH(1) model is the constant variance model.\n",
    "\n",
    "Given the assumption of normality, we can estimate the GARCH model parameters:\n",
    "\n",
    "$\\theta = [\\phi_0, \\alpha_0, \\alpha_1, ..., \\alpha_q,\\beta_1,\\beta_2,...,\\beta_p]$\n",
    "\n",
    "Using MLE. The log-likelihood is:\n",
    "\n",
    "$\\log L(\\theta) = \\frac{1}{T}\\sum^T_{t=1} \\log f(r_t|r_{t-1},r_{t-2},...;\\theta)$\n",
    "\n",
    "$=1\\frac{1}{2}\\log 2\\pi - \\frac{1}{2T}\\sum^T_{t=1} \\log h_t - \\frac{1}{2T} \\sum^T_{t=1} \\frac{u^2_t}{h_t}$\n",
    "\n",
    "$u_t = r_t-\\phi_0$\n",
    "\n",
    "$h_t = \\alpha_0 + \\sum^q_{i=1}\\alpha_i u^2_{t-1} + \\sum^p_{i=1} \\beta_i h_{t-i}$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2d04f120ef9720f2488447f7ea0097f595927923387bca235b2adec1aebe5795"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}